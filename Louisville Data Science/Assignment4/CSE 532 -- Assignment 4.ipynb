{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSE 532 Assignment 4 (Due 4/27/24)\n",
    "\n",
    "**Note: As with the previous assignment you should submit a separate document (.pdf or .doc(x)) with your responses to the analysis portion of the problems.** \n",
    "\n",
    "**1. (Machine Learning (Classification))** <br>a. Choose one of the [toy classification datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html) bundled with sklearn **other than the digits dataset**. <br> b. Train **three** distinct sklearn classification estimators for the chosen dataset and compare the results to see which one performs the best when using **2-fold cross-validation**.  Note that you should use three distinct classification models here (not just tweak underlying parameters).  A relatively complete listing of the available estimators can be found here (https://scikit-learn.org/stable/supervised_learning.html) -- but make sure you only use classifiers!  Unless you have an inclination to do otherwise, I recommend using the model default parameters when available.   <br> c. Repeat a. for **20-fold cross-validation**. Explain in a paragraph the difference in your results when using 20-fold vs 2-fold cross-validation (if any). <br>d. Construct a **confusion matrix** for your _most accurate_ model between the three estimators and two cross-fold options. <br> e. Which class in your dataset is most accurately predicted to have the correct label by the best classifier, and and which is most likely to be confused among one or more of the wrong classes?_(You can use a cell in a jupyter notebook file for this or a separate text/document file)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2-Fold) Mean R-squared score for KNeighborsClassifier: 0.94\n",
      "(2-Fold) Mean R-squared score for LinearRegressionClassifier: 0.9733333333333334\n",
      "(2-Fold) Mean R-squared score for DecisionTreeClassifier: 0.9266666666666667\n"
     ]
    }
   ],
   "source": [
    "# Part A and Part B\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "classifier1 = KNeighborsClassifier()\n",
    "classifier2 = LogisticRegression(max_iter=10000)\n",
    "classifier3 = DecisionTreeClassifier()\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=1)\n",
    "\n",
    "scores1 = cross_val_score(classifier1, X, y, cv=kf)\n",
    "scores2 = cross_val_score(classifier2, X, y, cv=kf)\n",
    "scores3 = cross_val_score(classifier3, X, y, cv=kf)\n",
    "\n",
    "print(\"(2-Fold) Mean R-squared score for KNeighborsClassifier:\", np.mean(scores1))\n",
    "print(\"(2-Fold) Mean R-squared score for LinearRegressionClassifier:\", np.mean(scores2))\n",
    "print(\"(2-Fold) Mean R-squared score for DecisionTreeClassifier:\", np.mean(scores3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression Classifier (linear model) seems to have the best average cross validation score, then the KNN classifier, and lastly the Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20-Fold) Mean R-squared score for KNeighborsClassifier: 0.9651785714285716\n",
      "(20-Fold) Mean R-squared score for LinearRegressionClassifier: 0.9660714285714287\n",
      "(20-Fold) Mean R-squared score for DecisionTreeClassifier: 0.9598214285714286\n"
     ]
    }
   ],
   "source": [
    "# Part C\n",
    "\n",
    "kf = KFold(n_splits=20, shuffle=True, random_state=1)\n",
    "\n",
    "scores1 = cross_val_score(classifier1, X, y, cv=kf)\n",
    "scores2 = cross_val_score(classifier2, X, y, cv=kf)\n",
    "scores3 = cross_val_score(classifier3, X, y, cv=kf)\n",
    "\n",
    "print(\"(20-Fold) Mean R-squared score for KNeighborsClassifier:\", np.mean(scores1))\n",
    "print(\"(20-Fold) Mean R-squared score for LinearRegressionClassifier:\", np.mean(scores2))\n",
    "print(\"(20-Fold) Mean R-squared score for DecisionTreeClassifier:\", np.mean(scores3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Difference in your results when using 20-fold vs 2-fold cross-validation:**\n",
    "\n",
    "It seems like both of sets of results when using the different number of cross-validation folds actually showed the same classifier order with regards to model accuracy (LRC > KNN > DTC). However, you can easily see that with more cross validation folds the arrays of accuracies seems to average out more (subsequent runs give around the same average acccuracy for the three models). A higher number of cross validation folds allows us to better predict how the model would perform with regards to different training and testing data subsets, giving us a better idea on how the model would actual perform on \"new\" and different testing data. Often, every bit of the training dataset will be used to train an actual model, leaving the model architect with the choice to use cross validation to find the optimal paramerters which give the model its best validation accuracy. Overall 20-fold cross validation gives us a better idea of how the model performs based on 20 different training and validation data splits, while 2-fold cross validation only provides us with 2 different training and validation splits leaving us with more model accuracy variation upcome consecutive executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  2 48]]\n"
     ]
    }
   ],
   "source": [
    "# Part D\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predicted = cross_val_predict(classifier2, X, y, cv=kf)\n",
    "matrix = confusion_matrix(y, predicted)\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E: Which class in your dataset is most accurately predicted to have the correct label by the best classifier, and and which is most likely to be confused among one or more of the wrong classes?**\n",
    "\n",
    "Class 0 is much more likely to be accurately predicted to have the correct label by my trained logistic regression classifier because it has been correctly classified 100% of the time and misclassified 0% of the time, it seems like class 0 has vastly different attributes when compared to the other two classes of iris flower types. Class 1 is the mostly likely class to be misclassified, in this case 3 samples (out of 50) were misclassified as class 2 when they actual were class 1 samples. However, it is tough to argue which class (class 1 or class 2) is misclassified the most because of the very small amount of samples that are available to validate with. I also think it is important to point out that class 1 samples and class 2 samples must have similar attributes to eachother because their misclassification trends specifically point to the opposing class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 (Option I). (Machine Learning (Regression))** <br>a. Locate a non-proprietary, small-scale dataset _suitable for regression_ online.  There are countless sources and repositories than you can use in this task, but if you have trouble finding one, I recommend starting via Kaggle (https://www.kaggle.com/code/rtatman/datasets-for-regression-analysis/notebook).  Explain briefly what the dataset represents, what target variable you will be using, and what other features are present.  _You may want or need to apply preprocessing to your data to insure it can be used properly with the regression models_ (e.g. making every feature numeric through transformation or by dropping some)  <br> b. Train **three** distinct sklearn regression estimators for the chosen dataset and compare the results to see which one performs the best when using **10-fold cross-validation**, utilizing the R-Squared score to gauge performance.  Note that you should use two distinct regression models here (not just tweak underlying parameters).  A relatively complete listing of the available estimators can be found here (https://scikit-learn.org/stable/supervised_learning.html) -- but make sure you only use regression models!  Unless you have an inclination to do otherwise, I recommend using the model default parameters when available.<br>  c. Repeat part b utilizing the Mean Square Error to gauge performance.  _Briefly_ research the difference between the two metrics (MSE and R2), and explain in a paragraph or two i. the difference between them ii. when each one is the preferable metric to use. _(You can use a cell in a jupyter notebook file for this or a separate text/document file)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chosen Dataset Summary**\n",
    "\n",
    "I chose a csv dataset that has several different attributes for each sample: transaction date (decimal), house age, distance to the nearest Mass Rapid Transit (MRT) station, number of convenience stores, longitude, and latitude. These features are being used to predict the price of the actual house per unit area (target variable). No preprocessing will be needed for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>X1 transaction date</th>\n",
       "      <th>X2 house age</th>\n",
       "      <th>X3 distance to the nearest MRT station</th>\n",
       "      <th>X4 number of convenience stores</th>\n",
       "      <th>X5 latitude</th>\n",
       "      <th>X6 longitude</th>\n",
       "      <th>Y house price of unit area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2012.917</td>\n",
       "      <td>32.0</td>\n",
       "      <td>84.87882</td>\n",
       "      <td>10</td>\n",
       "      <td>24.98298</td>\n",
       "      <td>121.54024</td>\n",
       "      <td>37.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2012.917</td>\n",
       "      <td>19.5</td>\n",
       "      <td>306.59470</td>\n",
       "      <td>9</td>\n",
       "      <td>24.98034</td>\n",
       "      <td>121.53951</td>\n",
       "      <td>42.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2013.583</td>\n",
       "      <td>13.3</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "      <td>47.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2013.500</td>\n",
       "      <td>13.3</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "      <td>54.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2012.833</td>\n",
       "      <td>5.0</td>\n",
       "      <td>390.56840</td>\n",
       "      <td>5</td>\n",
       "      <td>24.97937</td>\n",
       "      <td>121.54245</td>\n",
       "      <td>43.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>410</td>\n",
       "      <td>2013.000</td>\n",
       "      <td>13.7</td>\n",
       "      <td>4082.01500</td>\n",
       "      <td>0</td>\n",
       "      <td>24.94155</td>\n",
       "      <td>121.50381</td>\n",
       "      <td>15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>411</td>\n",
       "      <td>2012.667</td>\n",
       "      <td>5.6</td>\n",
       "      <td>90.45606</td>\n",
       "      <td>9</td>\n",
       "      <td>24.97433</td>\n",
       "      <td>121.54310</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>412</td>\n",
       "      <td>2013.250</td>\n",
       "      <td>18.8</td>\n",
       "      <td>390.96960</td>\n",
       "      <td>7</td>\n",
       "      <td>24.97923</td>\n",
       "      <td>121.53986</td>\n",
       "      <td>40.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>413</td>\n",
       "      <td>2013.000</td>\n",
       "      <td>8.1</td>\n",
       "      <td>104.81010</td>\n",
       "      <td>5</td>\n",
       "      <td>24.96674</td>\n",
       "      <td>121.54067</td>\n",
       "      <td>52.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>414</td>\n",
       "      <td>2013.500</td>\n",
       "      <td>6.5</td>\n",
       "      <td>90.45606</td>\n",
       "      <td>9</td>\n",
       "      <td>24.97433</td>\n",
       "      <td>121.54310</td>\n",
       "      <td>63.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      No  X1 transaction date  X2 house age  \\\n",
       "0      1             2012.917          32.0   \n",
       "1      2             2012.917          19.5   \n",
       "2      3             2013.583          13.3   \n",
       "3      4             2013.500          13.3   \n",
       "4      5             2012.833           5.0   \n",
       "..   ...                  ...           ...   \n",
       "409  410             2013.000          13.7   \n",
       "410  411             2012.667           5.6   \n",
       "411  412             2013.250          18.8   \n",
       "412  413             2013.000           8.1   \n",
       "413  414             2013.500           6.5   \n",
       "\n",
       "     X3 distance to the nearest MRT station  X4 number of convenience stores  \\\n",
       "0                                  84.87882                               10   \n",
       "1                                 306.59470                                9   \n",
       "2                                 561.98450                                5   \n",
       "3                                 561.98450                                5   \n",
       "4                                 390.56840                                5   \n",
       "..                                      ...                              ...   \n",
       "409                              4082.01500                                0   \n",
       "410                                90.45606                                9   \n",
       "411                               390.96960                                7   \n",
       "412                               104.81010                                5   \n",
       "413                                90.45606                                9   \n",
       "\n",
       "     X5 latitude  X6 longitude  Y house price of unit area  \n",
       "0       24.98298     121.54024                        37.9  \n",
       "1       24.98034     121.53951                        42.2  \n",
       "2       24.98746     121.54391                        47.3  \n",
       "3       24.98746     121.54391                        54.8  \n",
       "4       24.97937     121.54245                        43.1  \n",
       "..           ...           ...                         ...  \n",
       "409     24.94155     121.50381                        15.4  \n",
       "410     24.97433     121.54310                        50.0  \n",
       "411     24.97923     121.53986                        40.6  \n",
       "412     24.96674     121.54067                        52.5  \n",
       "413     24.97433     121.54310                        63.9  \n",
       "\n",
       "[414 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('real_estate.csv')\n",
    "# set X to data properties from columns X1-X6 \n",
    "X = df.iloc[:, 1:-1].values\n",
    "# set y to target variable (last column, house price of unit area)\n",
    "y = df.iloc[:, -1].values\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean R-squared score for KNeighborsRegressor: 0.5918522491710381\n",
      "Mean R-squared score for LinearRegression: 0.5704017534496113\n",
      "Mean R-squared score for RandomForestRegressor: 0.7015104791655438\n"
     ]
    }
   ],
   "source": [
    "#Part B\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "regressor1 = KNeighborsRegressor()\n",
    "regressor2 = LinearRegression()\n",
    "regressor3 = RandomForestRegressor()\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "scores1 = cross_val_score(regressor1, X, y, cv=kf, scoring=r2_scorer)\n",
    "scores2 = cross_val_score(regressor2, X, y, cv=kf, scoring=r2_scorer)\n",
    "scores3 = cross_val_score(regressor3, X, y, cv=kf, scoring=r2_scorer)\n",
    "\n",
    "print(\"Mean R-squared score for KNeighborsRegressor:\", np.mean(scores1))\n",
    "print(\"Mean R-squared score for LinearRegression:\", np.mean(scores2))\n",
    "print(\"Mean R-squared score for RandomForestRegressor:\", np.mean(scores3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RandomForestRegressor model seems to have the best average cross validation score, then the KNeighborsRegression model, and lastly the LinearRegression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Square Error for KNeighborsRegressor: 72.08382028985507\n",
      "Average Mean Square Error for LinearRegression: 80.3266248434177\n",
      "Average Mean Square Error for RandomForestRegressor: 56.472820973662806\n"
     ]
    }
   ],
   "source": [
    "# Part C\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "predict1 = cross_val_predict(regressor1, X, y, cv=kf)\n",
    "mse_scores1 = mean_squared_error(y, predict1)\n",
    "\n",
    "predict2 = cross_val_predict(regressor2, X, y, cv=kf)\n",
    "mse_scores2 = mean_squared_error(y, predict2)\n",
    "\n",
    "predict3 = cross_val_predict(regressor3, X, y, cv=kf)\n",
    "mse_scores3 = mean_squared_error(y, predict3)\n",
    "\n",
    "\n",
    "print(\"Average Mean Square Error for KNeighborsRegressor:\", mse_scores1)\n",
    "print(\"Average Mean Square Error for LinearRegression:\", mse_scores2)\n",
    "print(\"Average Mean Square Error for RandomForestRegressor:\", mse_scores3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the average MSE is the better the model is predicting. The RandomForestRegression model seems to have the lowest average MSE validation score, then the KNeighborsRegression model, and lastly the LinearRegression model has the highest average MSE. (same order as with R-squared values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Briefly research the difference between the two metrics (MSE and R2):** \n",
    "\n",
    "i. The difference between MSE and R-squared:\n",
    "\n",
    "The R-squared values measures how well the independent features can be used to predict or explain the target variable. The R-squared value ranges from 0 to 1, 0 meaning no correlation and 1 showing complete correlation. However, mean squared error is the average distance between the model's predicted output and the samples actual target output. Essentially, The smaller the MSE the better.\n",
    "\n",
    "ii. When each one is the preferable metric to use:\n",
    "\n",
    "When evaluating how well a model fits the dataset fed to it, you should utilize the GOF-based R-squared metric; however, if your main goal is to access the accuracy of a given model then MSE would be a better metric to utilize. Furthermore, if you want to emphasize the impact of outliers within your dataset when testing your model you would utilize the MSE metric, but if you wanted outliers to not have as much impact on your model's performance then you should utilize the R-squared metric. Overall, it is often beneficial to consider both metrics when analyzing a model.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
